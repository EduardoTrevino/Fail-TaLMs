{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def compute_cumulative_score(setting, model_name):\n",
    "    file_path = f\"./outputs/{setting}/{model_name}/answers.json\"\n",
    "    \n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    total_score = 0\n",
    "\n",
    "    for entry in data:\n",
    "        pass_rate = entry.get(\"pass_rate\", 0)\n",
    "        win_rate = entry.get(\"win_rate\", 0)\n",
    "        tool_annotation = entry.get(\"tool_annotation\", \"\")\n",
    "        info_annotation = entry.get(\"info_annotation\", \"\")\n",
    "\n",
    "        # Scoring for No-tools\n",
    "        if setting == \"No-tools\":\n",
    "            if win_rate >= 1:  # win (2) or tie (1)\n",
    "                total_score += 1\n",
    "        \n",
    "        # Scoring for Replaceable (Solvable)\n",
    "        elif setting == \"Replaceable\":\n",
    "            if tool_annotation == \"idk\":\n",
    "                total_score += 0.5\n",
    "                if pass_rate == 1:\n",
    "                    total_score += 1\n",
    "            elif pass_rate == 1:\n",
    "                total_score += 0.5\n",
    "\n",
    "        # Scoring for Non-Replaceable (Not solvable)\n",
    "        elif setting == \"Non-Replaceable\":\n",
    "            if tool_annotation == \"idk\":\n",
    "                total_score += 0.5\n",
    "                if pass_rate == 1:\n",
    "                    total_score += 1\n",
    "            elif tool_annotation == \"no\":\n",
    "                total_score += 0.5\n",
    "            if pass_rate == 1:\n",
    "                total_score += 0.5\n",
    "\n",
    "        # Scoring for Underspecified (Not solvable)\n",
    "        elif setting == \"Underspecified\":\n",
    "            if info_annotation == \"idk\":\n",
    "                total_score += 0.5\n",
    "                if pass_rate == 1:\n",
    "                    total_score += 1\n",
    "            elif info_annotation == \"no\":\n",
    "                total_score += 0.5\n",
    "            if pass_rate == 1:\n",
    "                total_score += 0.5\n",
    "\n",
    "    print(f\"Cumulative score for {model_name} under {setting}: {total_score:.2f}\")\n",
    "\n",
    "# Example function to compute scores for all settings for a given model\n",
    "def compute_scores_for_all_settings(model_name):\n",
    "    settings = [\"No-tools\", \"Replaceable\", \"Non-Replaceable\", \"Underspecified\"]\n",
    "    for setting in settings:\n",
    "        compute_cumulative_score(setting, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative score for claude3.5_sonnet_auto_eval under No-tools: 10.00\n",
      "Cumulative score for claude3.5_sonnet_auto_eval under Replaceable: 51.50\n",
      "Cumulative score for claude3.5_sonnet_auto_eval under Non-Replaceable: 48.00\n",
      "Cumulative score for claude3.5_sonnet_auto_eval under Underspecified: 40.50\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"claude3.5_sonnet_auto_eval\"  # Replace with the actual model name\n",
    "# model_name_2 = \"gpt_4o_auto_eval\"\n",
    "# model_name_3 = \"llama_70B_auto_eval\"\n",
    "# model_name_4 = \"llama_405B_auto_eval\"\n",
    "# Example usage\n",
    "model_name = \"claude3.5_sonnet_auto_eval\"  # Replace with your actual model name\n",
    "compute_scores_for_all_settings(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative score for llama_405B_auto_eval under No-tools: 28.00\n",
      "Cumulative score for llama_405B_auto_eval under Replaceable: 24.00\n",
      "Cumulative score for llama_405B_auto_eval under Non-Replaceable: 19.00\n",
      "Cumulative score for llama_405B_auto_eval under Underspecified: 17.00\n",
      "Total combined score for llama_405B_auto_eval across all settings: 88.00\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def compute_cumulative_score(setting, model_name):\n",
    "    file_path = f\"outputs/{setting}/{model_name}/answers.json\"\n",
    "    \n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    total_score = 0\n",
    "\n",
    "    for entry in data:\n",
    "        pass_rate = entry.get(\"pass_rate\", 0)\n",
    "        win_rate = entry.get(\"win_rate\", 0)\n",
    "        tool_annotation = entry.get(\"tool_annotation\", \"\")\n",
    "        info_annotation = entry.get(\"info_annotation\", \"\")\n",
    "\n",
    "        # Scoring for No-tools\n",
    "        if setting == \"No-tools\":\n",
    "            if win_rate >= 1:  # win (2) or tie (1)\n",
    "                total_score += 1\n",
    "        \n",
    "        # Scoring for Replaceable (Solvable)\n",
    "        elif setting == \"Replaceable\":\n",
    "            if tool_annotation == \"idk\":\n",
    "                total_score += 0.5\n",
    "                if pass_rate == 1:\n",
    "                    total_score += 1\n",
    "            elif pass_rate == 1:\n",
    "                total_score += 0.5\n",
    "\n",
    "        # Scoring for Non-Replaceable (Not solvable)\n",
    "        elif setting == \"Non-Replaceable\":\n",
    "            if tool_annotation == \"idk\":\n",
    "                total_score += 0.5\n",
    "                if pass_rate == 1:\n",
    "                    total_score += 1\n",
    "            elif tool_annotation == \"no\":\n",
    "                total_score += 0.5\n",
    "            if pass_rate == 1:\n",
    "                total_score += 0.5\n",
    "\n",
    "        # Scoring for Underspecified (Not solvable)\n",
    "        elif setting == \"Underspecified\":\n",
    "            if info_annotation == \"idk\":\n",
    "                total_score += 0.5\n",
    "                if pass_rate == 1:\n",
    "                    total_score += 1\n",
    "            elif info_annotation == \"no\":\n",
    "                total_score += 0.5\n",
    "            if pass_rate == 1:\n",
    "                total_score += 0.5\n",
    "\n",
    "    return total_score\n",
    "\n",
    "# Function to compute cumulative score across all settings for a given model\n",
    "def compute_total_score_across_settings(model_name):\n",
    "    settings = [\"No-tools\", \"Replaceable\", \"Non-Replaceable\", \"Underspecified\"]\n",
    "    total_score = 0\n",
    "\n",
    "    for setting in settings:\n",
    "        score = compute_cumulative_score(setting, model_name)\n",
    "        print(f\"Cumulative score for {model_name} under {setting}: {score:.2f}\")\n",
    "        total_score += score\n",
    "\n",
    "    # Print the combined total score across all settings\n",
    "    print(f\"Total combined score for {model_name} across all settings: {total_score:.2f}\")\n",
    "\n",
    "# Example usage\n",
    "model_name = \"llama_405B_auto_eval\"  # Replace with your actual model name\n",
    "compute_total_score_across_settings(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative score for claude3.5_sonnet_auto_eval under No-tools: 10.00\n",
      "Cumulative score for claude3.5_sonnet_auto_eval under Replaceable: 51.50\n",
      "Cumulative score for claude3.5_sonnet_auto_eval under Non-Replaceable: 48.00\n",
      "Cumulative score for claude3.5_sonnet_auto_eval under Underspecified: 40.50\n",
      "Total combined score for claude3.5_sonnet_auto_eval across all settings: 150.00\n",
      "Cumulative score for gpt_4o_auto_eval under No-tools: 29.00\n",
      "Cumulative score for gpt_4o_auto_eval under Replaceable: 23.50\n",
      "Cumulative score for gpt_4o_auto_eval under Non-Replaceable: 10.50\n",
      "Cumulative score for gpt_4o_auto_eval under Underspecified: 29.00\n",
      "Total combined score for gpt_4o_auto_eval across all settings: 92.00\n",
      "Cumulative score for llama_70B_auto_eval under No-tools: 37.00\n",
      "Cumulative score for llama_70B_auto_eval under Replaceable: 13.00\n",
      "Cumulative score for llama_70B_auto_eval under Non-Replaceable: 29.50\n",
      "Cumulative score for llama_70B_auto_eval under Underspecified: 22.50\n",
      "Total combined score for llama_70B_auto_eval across all settings: 102.00\n",
      "Cumulative score for llama_405B_auto_eval under No-tools: 28.00\n",
      "Cumulative score for llama_405B_auto_eval under Replaceable: 24.00\n",
      "Cumulative score for llama_405B_auto_eval under Non-Replaceable: 19.00\n",
      "Cumulative score for llama_405B_auto_eval under Underspecified: 17.00\n",
      "Total combined score for llama_405B_auto_eval across all settings: 88.00\n"
     ]
    }
   ],
   "source": [
    "model_name = \"claude3.5_sonnet_auto_eval\"  # Replace with the actual model name\n",
    "model_name_2 = \"gpt_4o_auto_eval\"\n",
    "model_name_3 = \"llama_70B_auto_eval\"\n",
    "model_name_4 = \"llama_405B_auto_eval\"\n",
    "\n",
    "compute_total_score_across_settings(model_name)\n",
    "compute_total_score_across_settings(model_name_2)\n",
    "compute_total_score_across_settings(model_name_3)\n",
    "compute_total_score_across_settings(model_name_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cumulative_score(setting, model_name):\n",
    "    file_path = f\"outputs/{setting}/{model_name}/answers.json\"\n",
    "    \n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    total_score = 0\n",
    "    total_queries = len(data)  # Count how many queries are in this setting\n",
    "\n",
    "    for entry in data:\n",
    "        pass_rate = entry.get(\"pass_rate\", 0)\n",
    "        win_rate = entry.get(\"win_rate\", 0)\n",
    "        tool_annotation = entry.get(\"tool_annotation\", \"\")\n",
    "        info_annotation = entry.get(\"info_annotation\", \"\")\n",
    "\n",
    "        # Scoring for No-tools\n",
    "        if setting == \"No-tools\":\n",
    "            if win_rate >= 1:  # win (2) or tie (1)\n",
    "                total_score += 1\n",
    "        \n",
    "        # Scoring for Replaceable (Solvable)\n",
    "        elif setting == \"Replaceable\":\n",
    "            if tool_annotation == \"idk\":\n",
    "                total_score += 0.2\n",
    "                if pass_rate == 1:\n",
    "                    total_score += 1\n",
    "            elif pass_rate == 1:\n",
    "                total_score += 0.5\n",
    "\n",
    "        # Scoring for Non-Replaceable (Not solvable)\n",
    "        elif setting == \"Non-Replaceable\":\n",
    "            if tool_annotation == \"idk\":\n",
    "                total_score += 0.5\n",
    "                if pass_rate == 1:\n",
    "                    total_score += 1\n",
    "            elif tool_annotation == \"no\":\n",
    "                total_score += 0.5\n",
    "            if pass_rate == 1:\n",
    "                total_score += 0.5\n",
    "\n",
    "        # Scoring for Underspecified (Not solvable)\n",
    "        elif setting == \"Underspecified\":\n",
    "            if info_annotation == \"idk\":\n",
    "                total_score += 0.5\n",
    "                if pass_rate == 1:\n",
    "                    total_score += 1\n",
    "            elif info_annotation == \"no\":\n",
    "                total_score += 0.5\n",
    "            if pass_rate == 1:\n",
    "                total_score += 0.5\n",
    "\n",
    "    return total_score, total_queries\n",
    "\n",
    "# Function to compute cumulative score across all settings for a given model\n",
    "def compute_total_score_across_settings(model_name):\n",
    "    settings = [\"No-tools\", \"Replaceable\", \"Non-Replaceable\", \"Underspecified\"]\n",
    "    total_score = 0\n",
    "    total_queries_across_all_settings = 0  # To store the total number of queries across all settings\n",
    "\n",
    "    for setting in settings:\n",
    "        score, queries = compute_cumulative_score(setting, model_name)\n",
    "        print(f\"Cumulative score for {model_name} under {setting}: {score:.2f}\")\n",
    "        total_score += score\n",
    "        total_queries_across_all_settings += queries\n",
    "\n",
    "    # Calculate the average score across all queries\n",
    "    if total_queries_across_all_settings > 0:\n",
    "        average_score = total_score / total_queries_across_all_settings\n",
    "    else:\n",
    "        average_score = 0\n",
    "\n",
    "    # Print the combined total score and the average score\n",
    "    print(f\"Total combined score for {model_name} across all settings: {total_score:.2f}\")\n",
    "    print(f\"Average score per query across all settings: {average_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative score for claude3.5_sonnet_auto_eval under No-tools: 10.00\n",
      "Cumulative score for claude3.5_sonnet_auto_eval under Replaceable: 35.90\n",
      "Cumulative score for claude3.5_sonnet_auto_eval under Non-Replaceable: 48.00\n",
      "Cumulative score for claude3.5_sonnet_auto_eval under Underspecified: 40.50\n",
      "Total combined score for claude3.5_sonnet_auto_eval across all settings: 134.40\n",
      "Average score per query across all settings: 0.35\n",
      "Cumulative score for gpt_4o_auto_eval under No-tools: 29.00\n",
      "Cumulative score for gpt_4o_auto_eval under Replaceable: 22.60\n",
      "Cumulative score for gpt_4o_auto_eval under Non-Replaceable: 10.50\n",
      "Cumulative score for gpt_4o_auto_eval under Underspecified: 29.00\n",
      "Total combined score for gpt_4o_auto_eval across all settings: 91.10\n",
      "Average score per query across all settings: 0.24\n",
      "Cumulative score for llama_70B_auto_eval under No-tools: 37.00\n",
      "Cumulative score for llama_70B_auto_eval under Replaceable: 8.20\n",
      "Cumulative score for llama_70B_auto_eval under Non-Replaceable: 29.50\n",
      "Cumulative score for llama_70B_auto_eval under Underspecified: 22.50\n",
      "Total combined score for llama_70B_auto_eval across all settings: 97.20\n",
      "Average score per query across all settings: 0.26\n",
      "Cumulative score for llama_405B_auto_eval under No-tools: 28.00\n",
      "Cumulative score for llama_405B_auto_eval under Replaceable: 23.40\n",
      "Cumulative score for llama_405B_auto_eval under Non-Replaceable: 19.00\n",
      "Cumulative score for llama_405B_auto_eval under Underspecified: 17.00\n",
      "Total combined score for llama_405B_auto_eval across all settings: 87.40\n",
      "Average score per query across all settings: 0.23\n"
     ]
    }
   ],
   "source": [
    "model_name = \"claude3.5_sonnet_auto_eval\"  # Replace with the actual model name\n",
    "model_name_2 = \"gpt_4o_auto_eval\"\n",
    "model_name_3 = \"llama_70B_auto_eval\"\n",
    "model_name_4 = \"llama_405B_auto_eval\"\n",
    "\n",
    "compute_total_score_across_settings(model_name)\n",
    "compute_total_score_across_settings(model_name_2)\n",
    "compute_total_score_across_settings(model_name_3)\n",
    "compute_total_score_across_settings(model_name_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- No-tools ---\n",
      "Total queries: 99\n",
      "Attempted queries: 99\n",
      "Succeeded queries (pass_rate == 1): 20\n",
      "Cumulative score for claude3.5_sonnet_auto_eval under No-tools: 10.00\n",
      "--- Replaceable ---\n",
      "Total queries: 97\n",
      "Attempted queries: 96\n",
      "Succeeded queries (pass_rate == 1): 40\n",
      "Cumulative score for claude3.5_sonnet_auto_eval under Replaceable: 51.50\n",
      "--- Non-Replaceable ---\n",
      "Total queries: 94\n",
      "Attempted queries: 86\n",
      "Succeeded queries (pass_rate == 1): 8\n",
      "Cumulative score for claude3.5_sonnet_auto_eval under Non-Replaceable: 48.00\n",
      "--- Underspecified ---\n",
      "Total queries: 95\n",
      "Attempted queries: 87\n",
      "Succeeded queries (pass_rate == 1): 29\n",
      "Cumulative score for claude3.5_sonnet_auto_eval under Underspecified: 40.50\n",
      "\n",
      "--- Overall ---\n",
      "Total queries across all settings: 385\n",
      "Attempted queries across all settings: 368\n",
      "Succeeded queries across all settings (pass_rate == 1): 97\n",
      "Total combined score for claude3.5_sonnet_auto_eval across all settings: 150.00\n",
      "Average score per query across all settings: 0.39\n",
      "--- No-tools ---\n",
      "Total queries: 99\n",
      "Attempted queries: 99\n",
      "Succeeded queries (pass_rate == 1): 57\n",
      "Cumulative score for gpt_4o_auto_eval under No-tools: 29.00\n",
      "--- Replaceable ---\n",
      "Total queries: 95\n",
      "Attempted queries: 94\n",
      "Succeeded queries (pass_rate == 1): 42\n",
      "Cumulative score for gpt_4o_auto_eval under Replaceable: 23.50\n",
      "--- Non-Replaceable ---\n",
      "Total queries: 95\n",
      "Attempted queries: 89\n",
      "Succeeded queries (pass_rate == 1): 10\n",
      "Cumulative score for gpt_4o_auto_eval under Non-Replaceable: 10.50\n",
      "--- Underspecified ---\n",
      "Total queries: 96\n",
      "Attempted queries: 91\n",
      "Succeeded queries (pass_rate == 1): 35\n",
      "Cumulative score for gpt_4o_auto_eval under Underspecified: 29.00\n",
      "\n",
      "--- Overall ---\n",
      "Total queries across all settings: 385\n",
      "Attempted queries across all settings: 373\n",
      "Succeeded queries across all settings (pass_rate == 1): 144\n",
      "Total combined score for gpt_4o_auto_eval across all settings: 92.00\n",
      "Average score per query across all settings: 0.24\n",
      "--- No-tools ---\n",
      "Total queries: 99\n",
      "Attempted queries: 99\n",
      "Succeeded queries (pass_rate == 1): 52\n",
      "Cumulative score for llama_70B_auto_eval under No-tools: 37.00\n",
      "--- Replaceable ---\n",
      "Total queries: 93\n",
      "Attempted queries: 75\n",
      "Succeeded queries (pass_rate == 1): 9\n",
      "Cumulative score for llama_70B_auto_eval under Replaceable: 13.00\n",
      "--- Non-Replaceable ---\n",
      "Total queries: 93\n",
      "Attempted queries: 58\n",
      "Succeeded queries (pass_rate == 1): 3\n",
      "Cumulative score for llama_70B_auto_eval under Non-Replaceable: 29.00\n",
      "--- Underspecified ---\n",
      "Total queries: 94\n",
      "Attempted queries: 84\n",
      "Succeeded queries (pass_rate == 1): 27\n",
      "Cumulative score for llama_70B_auto_eval under Underspecified: 22.50\n",
      "\n",
      "--- Overall ---\n",
      "Total queries across all settings: 379\n",
      "Attempted queries across all settings: 316\n",
      "Succeeded queries across all settings (pass_rate == 1): 91\n",
      "Total combined score for llama_70B_auto_eval across all settings: 101.50\n",
      "Average score per query across all settings: 0.27\n",
      "--- No-tools ---\n",
      "Total queries: 99\n",
      "Attempted queries: 99\n",
      "Succeeded queries (pass_rate == 1): 43\n",
      "Cumulative score for llama_405B_auto_eval under No-tools: 28.00\n",
      "--- Replaceable ---\n",
      "Total queries: 94\n",
      "Attempted queries: 94\n",
      "Succeeded queries (pass_rate == 1): 46\n",
      "Cumulative score for llama_405B_auto_eval under Replaceable: 24.00\n",
      "--- Non-Replaceable ---\n",
      "Total queries: 94\n",
      "Attempted queries: 91\n",
      "Succeeded queries (pass_rate == 1): 28\n",
      "Cumulative score for llama_405B_auto_eval under Non-Replaceable: 19.00\n",
      "--- Underspecified ---\n",
      "Total queries: 95\n",
      "Attempted queries: 89\n",
      "Succeeded queries (pass_rate == 1): 24\n",
      "Cumulative score for llama_405B_auto_eval under Underspecified: 17.00\n",
      "\n",
      "--- Overall ---\n",
      "Total queries across all settings: 382\n",
      "Attempted queries across all settings: 373\n",
      "Succeeded queries across all settings (pass_rate == 1): 141\n",
      "Total combined score for llama_405B_auto_eval across all settings: 88.00\n",
      "Average score per query across all settings: 0.23\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def compute_cumulative_score(setting, model_name):\n",
    "    file_path = f\"outputs/{setting}/{model_name}/answers.json\"\n",
    "    \n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    total_score = 0\n",
    "    total_queries = len(data)  # Total number of queries\n",
    "    attempted_queries = 0  # Queries where skipped is False\n",
    "    succeeded_queries = 0  # Queries where pass_rate == 1\n",
    "\n",
    "    for entry in data:\n",
    "        pass_rate = entry.get(\"pass_rate\", 0)\n",
    "        win_rate = entry.get(\"win_rate\", 0)\n",
    "        tool_annotation = entry.get(\"tool_annotation\", \"\")\n",
    "        info_annotation = entry.get(\"info_annotation\", \"\")\n",
    "        skipped = entry.get(\"skipped\", False)  # Check if the query was skipped\n",
    "\n",
    "        # Track attempted and succeeded queries\n",
    "        if not skipped:\n",
    "            attempted_queries += 1\n",
    "        if pass_rate == 1 and not skipped:\n",
    "            succeeded_queries += 1\n",
    "\n",
    "        # Scoring for No-tools\n",
    "        if setting == \"No-tools\":\n",
    "            if win_rate >= 1:  # win (2) or tie (1)\n",
    "                total_score += 1\n",
    "        \n",
    "        # Scoring for Replaceable (Solvable)\n",
    "        elif setting == \"Replaceable\":\n",
    "            if tool_annotation == \"idk\":\n",
    "                total_score += 0.5\n",
    "                if pass_rate == 1 and not skipped:\n",
    "                    total_score += 1\n",
    "            elif pass_rate == 1 and not skipped:\n",
    "                total_score += 0.5\n",
    "\n",
    "        # Scoring for Non-Replaceable (Not solvable)\n",
    "        elif setting == \"Non-Replaceable\":\n",
    "            if tool_annotation == \"idk\":\n",
    "                total_score += 0.5\n",
    "                if pass_rate == 1 and not skipped:\n",
    "                    total_score += 1\n",
    "            elif tool_annotation == \"no\":\n",
    "                total_score += 0.5\n",
    "            if pass_rate == 1 and not skipped:\n",
    "                total_score += 0.5\n",
    "\n",
    "        # Scoring for Underspecified (Not solvable)\n",
    "        elif setting == \"Underspecified\":\n",
    "            if info_annotation == \"idk\":\n",
    "                total_score += 0.5\n",
    "                if pass_rate == 1 and not skipped:\n",
    "                    total_score += 1\n",
    "            elif info_annotation == \"no\":\n",
    "                total_score += 0.5\n",
    "            if pass_rate == 1 and not skipped:\n",
    "                total_score += 0.5\n",
    "\n",
    "    return total_score, total_queries, attempted_queries, succeeded_queries\n",
    "\n",
    "# Function to compute cumulative score and additional stats across all settings for a given model\n",
    "def compute_total_score_across_settings(model_name):\n",
    "    settings = [\"No-tools\", \"Replaceable\", \"Non-Replaceable\", \"Underspecified\"]\n",
    "    total_score = 0\n",
    "    total_queries_across_all_settings = 0\n",
    "    total_attempted_across_all_settings = 0\n",
    "    total_succeeded_across_all_settings = 0\n",
    "\n",
    "    for setting in settings:\n",
    "        score, total_queries, attempted_queries, succeeded_queries = compute_cumulative_score(setting, model_name)\n",
    "        print(f\"--- {setting} ---\")\n",
    "        print(f\"Total queries: {total_queries}\")\n",
    "        print(f\"Attempted queries: {attempted_queries}\")\n",
    "        print(f\"Succeeded queries (pass_rate == 1): {succeeded_queries}\")\n",
    "        print(f\"Cumulative score for {model_name} under {setting}: {score:.2f}\")\n",
    "        \n",
    "        total_score += score\n",
    "        total_queries_across_all_settings += total_queries\n",
    "        total_attempted_across_all_settings += attempted_queries\n",
    "        total_succeeded_across_all_settings += succeeded_queries\n",
    "\n",
    "    # Calculate the average score across all queries\n",
    "    if total_queries_across_all_settings > 0:\n",
    "        average_score = total_score / total_queries_across_all_settings\n",
    "    else:\n",
    "        average_score = 0\n",
    "\n",
    "    # Print the combined total score and the average score\n",
    "    print(f\"\\n--- Overall ---\")\n",
    "    print(f\"Total queries across all settings: {total_queries_across_all_settings}\")\n",
    "    print(f\"Attempted queries across all settings: {total_attempted_across_all_settings}\")\n",
    "    print(f\"Succeeded queries across all settings (pass_rate == 1): {total_succeeded_across_all_settings}\")\n",
    "    print(f\"Total combined score for {model_name} across all settings: {total_score:.2f}\")\n",
    "    print(f\"Average score per query across all settings: {average_score:.2f}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "model_name = \"claude3.5_sonnet_auto_eval\"  # Replace with the actual model name\n",
    "model_name_2 = \"gpt_4o_auto_eval\"\n",
    "model_name_3 = \"llama_70B_auto_eval\"\n",
    "model_name_4 = \"llama_405B_auto_eval\"\n",
    "\n",
    "compute_total_score_across_settings(model_name)\n",
    "compute_total_score_across_settings(model_name_2)\n",
    "compute_total_score_across_settings(model_name_3)\n",
    "compute_total_score_across_settings(model_name_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Replaceable ---\n",
      "Scores: {'Task Success': 0.41237113402061853, 'Awareness Accuracy': 0.5360824742268041, 'Appropriate Action': 0.4144329896907217, 'Total Queries': 97}\n",
      "--- Non-Replaceable ---\n",
      "Scores: {'Task Success': 0.0851063829787234, 'Awareness Accuracy': 0.851063829787234, 'Appropriate Action': 0.1021276595744681, 'Total Queries': 94}\n",
      "--- Underspecified ---\n",
      "Scores: {'Task Success': 0.30526315789473685, 'Awareness Accuracy': 0.42105263157894735, 'Appropriate Action': 0.3221052631578947, 'Total Queries': 95}\n",
      "--- No-tools ---\n",
      "Scores: {'Task Success': 0.10101010101010101, 'Awareness Accuracy': None, 'Appropriate Action': 0.10101010101010101, 'Total Queries': 99}\n",
      "\n",
      "--- Overall ---\n",
      "Task Success Average: 0.23\n",
      "Awareness Accuracy Average: 0.60\n",
      "Appropriate Action Average: 0.23\n",
      "Overall Benchmark Score for claude3.5_sonnet_auto_eval: 0.34\n",
      "--- Replaceable ---\n",
      "Scores: {'Task Success': 0.4421052631578947, 'Awareness Accuracy': 0.031578947368421054, 'Appropriate Action': 0.4442105263157895, 'Total Queries': 95}\n",
      "--- Non-Replaceable ---\n",
      "Scores: {'Task Success': 0.10526315789473684, 'Awareness Accuracy': 0.09473684210526316, 'Appropriate Action': 0.11789473684210526, 'Total Queries': 95}\n",
      "--- Underspecified ---\n",
      "Scores: {'Task Success': 0.3645833333333333, 'Awareness Accuracy': 0.17708333333333334, 'Appropriate Action': 0.375, 'Total Queries': 96}\n",
      "--- No-tools ---\n",
      "Scores: {'Task Success': 0.29292929292929293, 'Awareness Accuracy': None, 'Appropriate Action': 0.29292929292929293, 'Total Queries': 99}\n",
      "\n",
      "--- Overall ---\n",
      "Task Success Average: 0.30\n",
      "Awareness Accuracy Average: 0.10\n",
      "Appropriate Action Average: 0.31\n",
      "Overall Benchmark Score for gpt_4o_auto_eval: 0.24\n",
      "--- Replaceable ---\n",
      "Scores: {'Task Success': 0.0967741935483871, 'Awareness Accuracy': 0.17204301075268819, 'Appropriate Action': 0.13548387096774187, 'Total Queries': 93}\n",
      "--- Non-Replaceable ---\n",
      "Scores: {'Task Success': 0.043010752688172046, 'Awareness Accuracy': 0.5483870967741935, 'Appropriate Action': 0.10752688172043011, 'Total Queries': 93}\n",
      "--- Underspecified ---\n",
      "Scores: {'Task Success': 0.2872340425531915, 'Awareness Accuracy': 0.19148936170212766, 'Appropriate Action': 0.3085106382978723, 'Total Queries': 94}\n",
      "--- No-tools ---\n",
      "Scores: {'Task Success': 0.37373737373737376, 'Awareness Accuracy': None, 'Appropriate Action': 0.37373737373737376, 'Total Queries': 99}\n",
      "\n",
      "--- Overall ---\n",
      "Task Success Average: 0.20\n",
      "Awareness Accuracy Average: 0.30\n",
      "Appropriate Action Average: 0.23\n",
      "Overall Benchmark Score for llama_70B_auto_eval: 0.24\n",
      "--- Replaceable ---\n",
      "Scores: {'Task Success': 0.48936170212765956, 'Awareness Accuracy': 0.02127659574468085, 'Appropriate Action': 0.48936170212765956, 'Total Queries': 94}\n",
      "--- Non-Replaceable ---\n",
      "Scores: {'Task Success': 0.2978723404255319, 'Awareness Accuracy': 0.0851063829787234, 'Appropriate Action': 0.30425531914893617, 'Total Queries': 94}\n",
      "--- Underspecified ---\n",
      "Scores: {'Task Success': 0.25263157894736843, 'Awareness Accuracy': 0.10526315789473684, 'Appropriate Action': 0.2652631578947368, 'Total Queries': 95}\n",
      "--- No-tools ---\n",
      "Scores: {'Task Success': 0.2828282828282828, 'Awareness Accuracy': None, 'Appropriate Action': 0.2828282828282828, 'Total Queries': 99}\n",
      "\n",
      "--- Overall ---\n",
      "Task Success Average: 0.33\n",
      "Awareness Accuracy Average: 0.07\n",
      "Appropriate Action Average: 0.34\n",
      "Overall Benchmark Score for llama_405B_auto_eval: 0.25\n",
      "+----------------------------+----------------+----------------------+----------------------+--------------+\n",
      "| Model                      |   Task Success |   Awareness Accuracy |   Appropriate Action |   SRUU Score |\n",
      "+============================+================+======================+======================+==============+\n",
      "| claude3.5_sonnet_auto_eval |           0.23 |                 0.6  |                 0.23 |         0.34 |\n",
      "+----------------------------+----------------+----------------------+----------------------+--------------+\n",
      "| gpt_4o_auto_eval           |           0.3  |                 0.1  |                 0.31 |         0.24 |\n",
      "+----------------------------+----------------+----------------------+----------------------+--------------+\n",
      "| llama_70B_auto_eval        |           0.2  |                 0.3  |                 0.23 |         0.24 |\n",
      "+----------------------------+----------------+----------------------+----------------------+--------------+\n",
      "| llama_405B_auto_eval       |           0.33 |                 0.07 |                 0.34 |         0.25 |\n",
      "+----------------------------+----------------+----------------------+----------------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tabulate import tabulate\n",
    "\n",
    "def compute_scores(setting, model_name):\n",
    "    file_path = f\"outputs/{setting}/{model_name}/answers.json\"\n",
    "    \n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    total_queries = len(data)\n",
    "    \n",
    "    # Initialize counters for each component\n",
    "    ts_score = 0  # Task Success\n",
    "    aa_score = 0  # Awareness Accuracy\n",
    "    aac_score = 0  # Appropriate Action\n",
    "    \n",
    "    for entry in data:\n",
    "        pass_rate = entry.get(\"pass_rate\", 0)\n",
    "        win_rate = entry.get(\"win_rate\", 0)\n",
    "        tool_annotation = entry.get(\"tool_annotation\", \"\").lower()\n",
    "        info_annotation = entry.get(\"info_annotation\", \"\").lower()\n",
    "        skipped = entry.get(\"skipped\", False)\n",
    "        \n",
    "        # Determine awareness\n",
    "        if setting in [\"Replaceable\", \"Non-Replaceable\"]:\n",
    "            awareness = tool_annotation\n",
    "        elif setting == \"Underspecified\":\n",
    "            awareness = info_annotation\n",
    "        else:\n",
    "            awareness = ''  # Not applicable\n",
    "\n",
    "        # Initialize variables\n",
    "        ts = 0\n",
    "        aa = 0\n",
    "        aac = 0\n",
    "        \n",
    "        if setting == \"No-tools\":\n",
    "            # Task Success\n",
    "            ts = 1 if win_rate >= 1 else 0\n",
    "            ts_score += ts\n",
    "\n",
    "            # Awareness Accuracy is not applicable\n",
    "            # Do not increment aa_score\n",
    "\n",
    "            # Appropriate Action\n",
    "            # The model should attempt to answer\n",
    "            if not skipped and win_rate >= 1:\n",
    "                aac = 1\n",
    "            else:\n",
    "                aac = 0\n",
    "            aac_score += aac\n",
    "\n",
    "        else:\n",
    "            # Task Success (TS)\n",
    "            ts = 1 if pass_rate == 1 else 0\n",
    "            ts_score += ts\n",
    "\n",
    "            # Awareness Accuracy (AA)\n",
    "            if setting == \"Replaceable\":\n",
    "                if awareness in [\"idk\"]:\n",
    "                    aa = 1\n",
    "            elif setting in [\"Non-Replaceable\", \"Underspecified\"]:\n",
    "                if awareness in [\"no\", \"idk\"]:\n",
    "                    aa = 1\n",
    "            aa_score += aa\n",
    "\n",
    "            # Appropriate Action (AAc)\n",
    "            if awareness in [\"no\", \"idk\"]:\n",
    "                if skipped:\n",
    "                    aac = 0.2\n",
    "                elif not skipped and pass_rate == 1:\n",
    "                    aac = 1\n",
    "                # Else, attempted but failed (pass_rate == 0), no points\n",
    "            elif awareness == \"yes\":\n",
    "                if not skipped and pass_rate == 1:\n",
    "                    aac = 1\n",
    "                # Else, skipped or failed, no points\n",
    "            aac_score += aac\n",
    "    \n",
    "    # Compute average scores\n",
    "    if total_queries > 0:\n",
    "        ts_avg = ts_score / total_queries\n",
    "        aac_avg = aac_score / total_queries\n",
    "        if setting == \"No-tools\":\n",
    "            aa_avg = None  # Awareness Accuracy not applicable\n",
    "        else:\n",
    "            aa_avg = aa_score / total_queries\n",
    "    else:\n",
    "        ts_avg = aa_avg = aac_avg = 0\n",
    "    \n",
    "    return {\n",
    "        'Task Success': ts_avg,\n",
    "        'Awareness Accuracy': aa_avg,\n",
    "        'Appropriate Action': aac_avg,\n",
    "        'Total Queries': total_queries\n",
    "    }\n",
    "\n",
    "def compute_overall_score(model_name):\n",
    "    settings = [\"Replaceable\", \"Non-Replaceable\", \"Underspecified\", \"No-tools\"]\n",
    "    \n",
    "    ts_scores = []\n",
    "    aa_scores = []\n",
    "    aac_scores = []\n",
    "    total_queries = 0\n",
    "    \n",
    "    for setting in settings:\n",
    "        scores = compute_scores(setting, model_name)\n",
    "        print(f\"--- {setting} ---\")\n",
    "        print(f\"Scores: {scores}\")\n",
    "        total_queries += scores['Total Queries']\n",
    "        \n",
    "        ts_scores.append(scores['Task Success'])\n",
    "        if scores['Awareness Accuracy'] is not None:\n",
    "            aa_scores.append(scores['Awareness Accuracy'])\n",
    "        aac_scores.append(scores['Appropriate Action'])\n",
    "    \n",
    "    # Compute overall averages\n",
    "    ts_overall = sum(ts_scores) / len(ts_scores) if ts_scores else 0\n",
    "    aa_overall = sum(aa_scores) / len(aa_scores) if aa_scores else 0\n",
    "    aac_overall = sum(aac_scores) / len(aac_scores) if aac_scores else 0\n",
    "    \n",
    "    # Weights\n",
    "    w_ts = 0.4  # Task Success\n",
    "    w_aa = 0.3  # Awareness Accuracy\n",
    "    w_aac = 0.3  # Appropriate Action\n",
    "    \n",
    "    # Compute overall score\n",
    "    overall_score = (w_ts * ts_overall) + (w_aa * aa_overall) + (w_aac * aac_overall)\n",
    "    \n",
    "    print(f\"\\n--- Overall ---\")\n",
    "    print(f\"Task Success Average: {ts_overall:.2f}\")\n",
    "    print(f\"Awareness Accuracy Average: {aa_overall:.2f}\")\n",
    "    print(f\"Appropriate Action Average: {aac_overall:.2f}\")\n",
    "    print(f\"Overall Benchmark Score for {model_name}: {overall_score:.2f}\")\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'Task Success': ts_overall,\n",
    "        'Awareness Accuracy': aa_overall,\n",
    "        'Appropriate Action': aac_overall,\n",
    "        'Overall Score': overall_score\n",
    "    }\n",
    "\n",
    "# Collecting results for all models\n",
    "model_names = [\n",
    "    \"claude3.5_sonnet_auto_eval\",\n",
    "    \"gpt_4o_auto_eval\",\n",
    "    \"llama_70B_auto_eval\",\n",
    "    \"llama_405B_auto_eval\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Define the benchmark name\n",
    "benchmark_name = \"SRUU\"  # Replace with your chosen acronym\n",
    "\n",
    "for model_name in model_names:\n",
    "    result = compute_overall_score(model_name)\n",
    "    results.append(result)\n",
    "\n",
    "# Prepare data for the table\n",
    "table_data = []\n",
    "headers = [\"Model\", \"Task Success\", \"Awareness Accuracy\", \"Appropriate Action\", f\"{benchmark_name} Score\"]\n",
    "\n",
    "for res in results:\n",
    "    table_data.append([\n",
    "        res['Model'],\n",
    "        f\"{res['Task Success']:.2f}\",\n",
    "        f\"{res['Awareness Accuracy']:.2f}\" if res['Awareness Accuracy'] is not None else \"N/A\",\n",
    "        f\"{res['Appropriate Action']:.2f}\",\n",
    "        f\"{res['Overall Score']:.2f}\"\n",
    "    ])\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toolsforjob",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
